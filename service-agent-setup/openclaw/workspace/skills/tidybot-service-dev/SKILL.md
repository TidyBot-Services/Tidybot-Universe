# TidyBot Service Development

Build and deploy a backend ML service for the TidyBot ecosystem.

## Before You Start

1. Read `docs/CLIENT_SDK_SPEC.md` — every client SDK must follow this spec
2. Check `catalog.json` for the next available port
3. Check GPU VRAM usage with `nvidia-smi` — make sure there's room for your model

## Step-by-Step Build Workflow

### 1. Create the repo

```bash
gh repo create TidyBot-Services/<service-name> --public --clone
cd <service-name>
```

### 2. Set up Python environment

```bash
python3 -m venv venv
source venv/bin/activate
pip install 'numpy<2' torch torchvision fastapi uvicorn pydantic
```

### 3. Build `main.py` (FastAPI server)

Every service MUST have:

- **`GET /health`** → `{"status": "ok"}` — health check endpoint
- **`POST /<action>`** — primary inference endpoint accepting base64-encoded image/data
- **`GET /docs`** — auto-generated by FastAPI (Swagger UI)
- **Lifespan handler** — load model weights at startup, not per-request
- **CUDA device selection** — respect `CUDA_VISIBLE_DEVICES` env var
- **Error handling** — return proper HTTP error codes, not crash

Template structure:

```python
"""<Service Name> — TidyBot Backend Service"""
import base64, io, torch
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global MODEL
    MODEL = load_your_model()  # Load once at startup
    yield

app = FastAPI(title="<Service Name>", lifespan=lifespan)

@app.get("/health")
def health():
    return {"status": "ok"}

class PredictRequest(BaseModel):
    image: str  # base64 encoded

@app.post("/predict")
def predict(req: PredictRequest):
    img_bytes = base64.b64decode(req.image)
    # ... run inference ...
    return {"results": [...]}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=<PORT>)
```

### 4. Build `client.py` (Client SDK)

**Must follow `CLIENT_SDK_SPEC.md` exactly:**

- Use `urllib.request` only (NOT `requests`)
- Accept `bytes` input for images/data
- Include `health()` → `bool` method
- Constructor takes `host` parameter
- Type hints and docstrings on all public methods

See `docs/CLIENT_SDK_SPEC.md` for the full template.

### 5. Test locally

```bash
# Start the server
python main.py &

# Test health
curl http://localhost:<PORT>/health

# Test with a real image
python -c "
from client import Client
c = Client('http://localhost:<PORT>')
print('Health:', c.health())
with open('test.jpg', 'rb') as f:
    result = c.predict(f.read())
print(result)
"
```

### 6. Write README.md

Include:
- What the service does
- API endpoints with request/response examples
- Hardware requirements (VRAM, RAM)
- Model details (architecture, weights source, license)
- Client SDK usage examples

### 7. Push to GitHub

```bash
# Freeze deps
pip freeze > requirements.txt
echo "venv/" > .gitignore
echo "*.pt" >> .gitignore
echo "*.pth" >> .gitignore

git add -A
git commit -m "Initial <service-name> service"
git push -u origin main
```

### 8. Deploy as systemd unit

```bash
sudo tee /etc/systemd/system/tidybot-<name>.service << EOF
[Unit]
Description=TidyBot <Name> Service
After=network.target

[Service]
Type=simple
User=$(whoami)
WorkingDirectory=$(pwd)
ExecStart=$(pwd)/venv/bin/python main.py
Restart=on-failure
RestartSec=10
Environment=CUDA_VISIBLE_DEVICES=0

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable tidybot-<name>.service
sudo systemctl start tidybot-<name>.service

# Verify
sudo systemctl status tidybot-<name>.service
curl http://localhost:<PORT>/health
```

### 9. Update catalog.json

Add entry to `TidyBot-Services/services_wishlist/catalog.json`:

```json
{
  "<service-name>": {
    "type": "model",
    "description": "...",
    "service_repo": "https://github.com/TidyBot-Services/<service-name>",
    "client_sdk": "https://raw.githubusercontent.com/TidyBot-Services/<service-name>/main/client.py",
    "api_docs": "https://github.com/TidyBot-Services/<service-name>/blob/main/README.md",
    "host": "http://<SERVER_IP>:<PORT>",
    "endpoints": ["GET /health", "POST /predict", "GET /docs"],
    "models": ["<model-name>"],
    "added_by": "<your-agent-name>",
    "added_at": "<YYYY-MM-DD>",
    "version": "0.1.0",
    "usage": {
      "import": "from client import Client",
      "init": "client = Client('http://<SERVER_IP>:<PORT>')",
      "example": "result = client.predict(image_bytes)",
      "returns": "dict with prediction results"
    }
  }
}
```

### 10. Update wishlist.json

Set the item's status to `done`, add `completed_at` timestamp.

## Common Pitfalls

- **Forgetting `numpy<2`** — torch breaks with numpy 2.x
- **Using `requests` in client.py** — must use `urllib` per spec
- **Forgetting to update BOTH catalog.json AND wishlist.json** — always update both
- **Using nohup instead of systemd** — services must be systemd units
- **Using `--now` with systemctl** — use separate `enable` + `start` commands
- **Not testing the client SDK** — always test client.py against the running server
- **Hardcoding server IP in client.py** — must be a constructor parameter
- **Loading models per-request** — use lifespan/startup handler to load once
